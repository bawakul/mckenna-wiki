---
phase: 01-corpus-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - supabase/migrations/001_create_corpus_tables.sql
  - supabase/migrations/002_create_search_function.sql
  - scripts/seed/import-corpus.ts
  - package.json
autonomous: true
user_setup:
  - service: supabase
    why: "Database for transcript storage and full-text search"
    env_vars:
      - name: SUPABASE_URL
        source: "Supabase Dashboard -> Project Settings -> API -> Project URL"
      - name: SUPABASE_ANON_KEY
        source: "Supabase Dashboard -> Project Settings -> API -> anon/public key"
      - name: SUPABASE_SERVICE_KEY
        source: "Supabase Dashboard -> Project Settings -> API -> service_role key"
    dashboard_config:
      - task: "Create a new Supabase project (free tier is fine)"
        location: "https://supabase.com/dashboard -> New Project"

must_haves:
  truths:
    - "Supabase database has transcripts and transcript_paragraphs tables with all required columns"
    - "Generated tsvector columns exist on both tables with GIN indexes for fast search"
    - "search_paragraphs RPC function returns ranked results with transcript metadata"
    - "Seed script reads JSON files from corpus directory and upserts into Supabase"
    - "Seed script uses content hashes to skip unchanged transcripts on re-import"
  artifacts:
    - path: "supabase/migrations/001_create_corpus_tables.sql"
      provides: "Database schema with tables, generated tsvector columns, and GIN indexes"
      contains: "CREATE TABLE transcripts"
      min_lines: 40
    - path: "supabase/migrations/002_create_search_function.sql"
      provides: "Full-text search RPC function"
      contains: "search_paragraphs"
      min_lines: 20
    - path: "scripts/seed/import-corpus.ts"
      provides: "Seed script to import JSON corpus into Supabase"
      min_lines: 80
  key_links:
    - from: "scripts/seed/import-corpus.ts"
      to: "@supabase/supabase-js"
      via: "createClient with service key"
      pattern: "createClient.*SUPABASE"
    - from: "scripts/seed/import-corpus.ts"
      to: "corpus JSON files"
      via: "readdirSync and JSON.parse"
      pattern: "readFileSync|readdirSync"
    - from: "supabase/migrations/001_create_corpus_tables.sql"
      to: "supabase/migrations/002_create_search_function.sql"
      via: "search function references tables from migration 001"
      pattern: "transcript_paragraphs"
---

<objective>
Create the Supabase database schema with full-text search support and build the seed script that imports JSON corpus files into the database.

Purpose: This is the storage and retrieval layer. The database schema defines how transcripts and paragraphs are stored, and the full-text search infrastructure (generated tsvector columns, GIN indexes, search RPC) ensures sub-200ms search performance. The seed script bridges the JSON corpus files to the database.

Output: SQL migrations ready to apply, seed script ready to import corpus data, full-text search function ready to query.
</objective>

<execution_context>
@/Users/bharadwajkulkarni/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bharadwajkulkarni/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-corpus-foundation/01-CONTEXT.md
@.planning/phases/01-corpus-foundation/01-RESEARCH.md
@.planning/phases/01-corpus-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Supabase database schema with full-text search</name>
  <files>supabase/migrations/001_create_corpus_tables.sql, supabase/migrations/002_create_search_function.sql</files>
  <action>
    1. Create `supabase/migrations/001_create_corpus_tables.sql`:

       **transcripts table:**
       - id: TEXT PRIMARY KEY (URL slug from organism.earth)
       - url: TEXT NOT NULL (full source URL)
       - title: TEXT NOT NULL
       - date: TEXT (stored as-is from source -- may be just year, or full date)
       - location: TEXT
       - speakers: TEXT[] (array of speaker names)
       - duration_minutes: INTEGER
       - word_count: INTEGER
       - topic_tags: TEXT[] (array of topic/tag strings)
       - referenced_authors: TEXT[] (array of referenced author names)
       - description: TEXT (transcript summary/description if available)
       - content_hash: TEXT NOT NULL (SHA-256 of normalized content for change detection)
       - scraped_at: TIMESTAMPTZ NOT NULL
       - created_at: TIMESTAMPTZ DEFAULT NOW()
       - updated_at: TIMESTAMPTZ DEFAULT NOW()
       - search_vector: tsvector GENERATED ALWAYS AS (
           setweight(to_tsvector('english', COALESCE(title, '')), 'A') ||
           setweight(to_tsvector('english', COALESCE(description, '')), 'B') ||
           setweight(to_tsvector('english', COALESCE(array_to_string(topic_tags, ' '), '')), 'C') ||
           setweight(to_tsvector('english', COALESCE(array_to_string(referenced_authors, ' '), '')), 'C')
         ) STORED

       **transcript_paragraphs table:**
       - id: SERIAL PRIMARY KEY
       - transcript_id: TEXT REFERENCES transcripts(id) ON DELETE CASCADE
       - position: INTEGER NOT NULL (zero-indexed sequential order)
       - speaker: TEXT (null if not identified)
       - timestamp: TEXT (null if not available; stored as-is from source)
       - text: TEXT NOT NULL
       - content_hash: TEXT NOT NULL (first 16 hex chars of SHA-256 for anchoring)
       - search_vector: tsvector GENERATED ALWAYS AS (
           to_tsvector('english', COALESCE(text, ''))
         ) STORED
       - UNIQUE(transcript_id, position)

       **Indexes:**
       - GIN index on transcripts.search_vector
       - GIN index on transcript_paragraphs.search_vector
       - Index on transcript_paragraphs.transcript_id (for fast paragraph lookups by transcript)
       - Index on transcripts.date (for chronological sorting)

       **updated_at trigger:**
       - Create a trigger function that sets updated_at = NOW() on UPDATE
       - Attach to transcripts table

    2. Create `supabase/migrations/002_create_search_function.sql`:

       **search_paragraphs RPC function:**
       ```sql
       CREATE OR REPLACE FUNCTION search_paragraphs(
         search_query TEXT,
         result_limit INTEGER DEFAULT 20
       )
       RETURNS TABLE (
         transcript_id TEXT,
         transcript_title TEXT,
         transcript_date TEXT,
         paragraph_position INTEGER,
         speaker TEXT,
         paragraph_text TEXT,
         rank REAL
       )
       ```
       - Uses websearch_to_tsquery('english', search_query) for user-friendly search syntax
       - JOINs transcript_paragraphs with transcripts to include transcript metadata
       - Orders by rank DESC, transcript date DESC, position ASC
       - Limits to result_limit

       **search_transcripts RPC function:**
       ```sql
       CREATE OR REPLACE FUNCTION search_transcripts(
         search_query TEXT,
         result_limit INTEGER DEFAULT 20
       )
       RETURNS TABLE (
         id TEXT,
         title TEXT,
         date TEXT,
         word_count INTEGER,
         topic_tags TEXT[],
         rank REAL
       )
       ```
       - Searches transcript-level metadata (title, description, tags, authors)
       - For finding specific lectures by topic

    These are SQL files to be run manually in the Supabase SQL editor or via Supabase CLI. They are NOT automatic migrations -- the user will apply them after creating their Supabase project.
  </action>
  <verify>
    - `supabase/migrations/001_create_corpus_tables.sql` exists with CREATE TABLE statements for both tables
    - SQL includes GENERATED ALWAYS AS for tsvector columns
    - SQL includes CREATE INDEX ... USING gin() for both search vectors
    - `supabase/migrations/002_create_search_function.sql` exists with search_paragraphs and search_transcripts functions
    - SQL is syntactically valid (no obvious typos in keywords)
    - UNIQUE constraint on (transcript_id, position)
  </verify>
  <done>Database schema SQL files are complete with tables, generated tsvector columns, GIN indexes, and search RPC functions</done>
</task>

<task type="auto">
  <name>Task 2: Build seed script for corpus import</name>
  <files>scripts/seed/import-corpus.ts, package.json</files>
  <action>
    1. Create `scripts/seed/import-corpus.ts`:
       - Reads CORPUS_REPO_PATH from environment variable (defaults to `./mckenna-corpus`)
       - CLI args: `--corpus-path PATH` (overrides env var), `--dry-run` (validate without writing to DB)
       - Workflow:
         a. List all `.json` files in corpus path's `transcripts/` directory
         b. For each JSON file:
            - Parse and validate structure (expected fields: id, title, paragraphs[], contentHash, etc.)
            - Check if transcript already exists in Supabase with same content_hash
            - If hash matches: skip (log "unchanged, skipping")
            - If hash differs or transcript is new: upsert transcript metadata, then upsert all paragraphs
            - When upserting paragraphs: delete existing paragraphs for this transcript first (in case count changed), then insert new ones. Use a transaction or delete+insert pattern.
         c. Error handling: log errors per transcript, continue with rest, report summary
         d. Summary output: total files, new imports, updated (hash changed), skipped (unchanged), failed

       - Supabase client: use service_role key (not anon key) since this is a server-side admin script
       - Batch paragraph inserts: chunk into groups of 50 to avoid payload size limits

    2. Add package.json scripts:
       ```
       "seed": "npx tsx scripts/seed/import-corpus.ts",
       "seed:dry-run": "npx tsx scripts/seed/import-corpus.ts --dry-run"
       ```

    3. The seed script should NOT depend on importing types from `scripts/scrape/types.ts` directly -- instead define its own minimal interface for the JSON file shape. This keeps the seed script independent of the scraper (important since CONTEXT.md says scraper is in app repo but corpus is separate data).

    IMPORTANT: The seed script needs Supabase credentials in .env.local. It should give a clear error message if credentials are missing: "Missing SUPABASE_URL or SUPABASE_SERVICE_KEY. See .env.example for required variables."
  </action>
  <verify>
    - `scripts/seed/import-corpus.ts` exists and compiles: `npx tsx --eval "import './scripts/seed/import-corpus'"` (may error on missing env vars, but should not have syntax errors)
    - Script checks for required env vars and gives clear error if missing
    - Script handles both `--corpus-path` CLI arg and CORPUS_REPO_PATH env var
    - Script has hash-based skip logic (grep for "content_hash" comparison)
    - Script chunks paragraph inserts (grep for chunk/batch logic)
    - `package.json` has "seed" and "seed:dry-run" scripts
  </verify>
  <done>Seed script is complete: reads JSON corpus files, validates structure, upserts into Supabase with hash-based change detection, handles errors gracefully</done>
</task>

</tasks>

<verification>
- SQL migration files are syntactically valid and create the expected schema
- Seed script compiles without syntax errors
- Seed script gives clear error when Supabase credentials are missing
- Both search RPC functions are defined and reference correct tables/columns
- GIN indexes are created for both tsvector columns
</verification>

<success_criteria>
1. Two SQL migration files exist covering: tables, generated columns, GIN indexes, triggers, search functions
2. Seed script reads JSON files and upserts into Supabase with hash-based change detection
3. Seed script handles errors per-transcript without aborting the batch
4. All database columns match the metadata fields defined in CONTEXT.md and RESEARCH.md
</success_criteria>

<output>
After completion, create `.planning/phases/01-corpus-foundation/01-03-SUMMARY.md`
</output>
