---
phase: 01-corpus-foundation
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - scripts/scrape/scraper.ts
  - scripts/seed/import-corpus.ts
  - scripts/verify-corpus.ts
  - package.json
autonomous: false

must_haves:
  truths:
    - "All ~90 transcripts from organism.earth are scraped and stored as JSON files"
    - "All JSON files are imported into Supabase with correct metadata and paragraph structure"
    - "Full-text search returns relevant results in under 200ms"
    - "Content hashes enable change detection (re-running seed skips unchanged transcripts)"
    - "Corpus JSON files live in a separate private GitHub repository"
  artifacts:
    - path: "scripts/verify-corpus.ts"
      provides: "Verification script that checks corpus completeness and search performance"
      min_lines: 60
  key_links:
    - from: "scripts/scrape/scraper.ts"
      to: "corpus JSON files"
      via: "writeFileSync to output directory"
      pattern: "writeFileSync"
    - from: "scripts/seed/import-corpus.ts"
      to: "Supabase transcripts table"
      via: "upsert with service key"
      pattern: "upsert.*transcripts"
    - from: "scripts/verify-corpus.ts"
      to: "Supabase search_paragraphs"
      via: "RPC call to verify search performance"
      pattern: "rpc.*search_paragraphs"
---

<objective>
Run the full scrape of all ~90 transcripts, create the private corpus repository, apply database migrations, seed the database, and verify everything works end-to-end including full-text search performance.

Purpose: This is the integration and verification plan. All prior plans built individual components (scraper, schema, seed script). This plan runs everything together, creates the corpus repository, and verifies the phase success criteria are met.

Output: Complete corpus in Supabase, private corpus repo on GitHub, verification that all 5 phase success criteria are satisfied.
</objective>

<execution_context>
@/Users/bharadwajkulkarni/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bharadwajkulkarni/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-corpus-foundation/01-CONTEXT.md
@.planning/phases/01-corpus-foundation/01-RESEARCH.md
@.planning/phases/01-corpus-foundation/01-01-SUMMARY.md
@.planning/phases/01-corpus-foundation/01-02-SUMMARY.md
@.planning/phases/01-corpus-foundation/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create corpus repo, run full scrape, apply migrations, and seed database</name>
  <files>package.json</files>
  <action>
    **Step 1: Create private corpus repository**
    - Create a new private GitHub repo: `gh repo create mckenna-corpus --private --description "McKenna lecture transcripts corpus (private, not for redistribution)" --clone`
    - Clone it into a known location (either as sibling directory or submodule)
    - Create directory structure inside: `mckenna-corpus/transcripts/`
    - Add a minimal README.md: "Private corpus of McKenna lecture transcripts from organism.earth. Not for redistribution."
    - Set CORPUS_REPO_PATH in `.env.local` to point to the corpus repo's path

    **Step 2: Run full scrape**
    - Run `npm run scrape` (no --limit flag = all transcripts)
    - Set scraper output directory to `{CORPUS_REPO_PATH}/transcripts/`
    - This will take approximately 3-5 minutes (90 transcripts x 2-3 second delays)
    - Monitor output: watch for failed transcripts, note total count
    - If any transcripts fail: run scraper again with just the failed URLs (or re-run full batch since it overwrites existing files)

    **Step 3: Validate scraped corpus**
    - Count JSON files: should be ~90
    - Spot-check 3-5 files from different parts of the alphabet
    - Verify word count seems reasonable (expect total ~1.3M words across all files)

    **Step 4: Commit corpus to repo**
    - `cd {CORPUS_REPO_PATH} && git add . && git commit -m "Initial corpus: ~90 McKenna transcripts from organism.earth" && git push`

    **Step 5: Apply database migrations**
    - The user needs to have created a Supabase project and added credentials to .env.local
    - Apply migrations by running the SQL in the Supabase SQL editor:
      - First: `supabase/migrations/001_create_corpus_tables.sql`
      - Then: `supabase/migrations/002_create_search_function.sql`
    - Alternatively, use the Supabase CLI if installed: `supabase db push`
    - Verify tables exist by querying: `SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'`

    **Step 6: Seed database**
    - Run `npm run seed`
    - Monitor output: expect ~90 transcripts imported, 0 failed, 0 skipped (first import)
    - Re-run `npm run seed` a second time to verify hash-based skip logic works (all should show as "unchanged, skipping")

    IMPORTANT: This task requires Supabase credentials in .env.local. If the user hasn't set up Supabase yet, Steps 5 and 6 will fail. The task should handle this gracefully and provide clear instructions.

    IMPORTANT: The full scrape takes 3-5 minutes. Use a timeout of at least 600 seconds for the scrape command.
  </action>
  <verify>
    - Corpus repo exists on GitHub: `gh repo view mckenna-corpus`
    - JSON file count: `ls {CORPUS_REPO_PATH}/transcripts/*.json | wc -l` shows ~90
    - Supabase tables exist (query information_schema)
    - `npm run seed` completes: all transcripts imported
    - Second `npm run seed` shows all transcripts skipped (hash unchanged)
  </verify>
  <done>Full corpus scraped, stored in private GitHub repo, imported into Supabase, and hash-based change detection verified</done>
</task>

<task type="auto">
  <name>Task 2: Create verification script and validate phase success criteria</name>
  <files>scripts/verify-corpus.ts, package.json</files>
  <action>
    1. Create `scripts/verify-corpus.ts` -- a comprehensive verification script that checks all phase success criteria:

       **Check 1: Corpus completeness**
       - Query Supabase: `SELECT COUNT(*) FROM transcripts` -- expect ~90
       - Query: `SELECT COUNT(*) FROM transcript_paragraphs` -- expect thousands
       - Query: `SELECT SUM(word_count) FROM transcripts` -- expect ~1.3M
       - Log: transcript count, paragraph count, total word count

       **Check 2: Metadata completeness**
       - Query: count of transcripts with non-null title, date, speakers, topic_tags
       - Log: percentage of transcripts with each metadata field populated
       - Flag if title is null for any transcript (should be 100%)

       **Check 3: Paragraph structure**
       - Query: verify every transcript has at least 1 paragraph
       - Query: verify all paragraphs have position, text, and content_hash
       - Check: longest transcript paragraph count (expect highest to be several hundred)
       - Check: no orphaned paragraphs (paragraphs without valid transcript_id)

       **Check 4: Full-text search performance**
       - Run 5 test searches using the search_paragraphs RPC:
         - "novelty theory" (core McKenna concept)
         - "psychedelic experience" (common topic)
         - "archaic revival" (book/lecture title)
         - "timewave" (specific concept)
         - "machine elves" (well-known McKenna term)
       - For each: log result count and execution time
       - Verify all queries complete in under 200ms
       - Verify each query returns at least 1 result (these are known McKenna topics)

       **Check 5: Content hashes**
       - Query: verify all transcripts have non-null content_hash
       - Query: verify all content_hash values are unique (no duplicate hashes)
       - Query: verify all paragraph content_hashes are 16 chars

       **Check 6: Corpus repo**
       - Verify CORPUS_REPO_PATH is set and directory exists
       - Count JSON files in corpus repo
       - Verify count matches database count

       **Output:** Print a summary table with PASS/FAIL for each check. Exit with code 0 if all pass, code 1 if any fail.

    2. Add to package.json: `"verify:corpus": "npx tsx scripts/verify-corpus.ts"`

    3. Run the verification: `npm run verify:corpus`

    4. If any checks fail, investigate and fix the root cause (re-scrape, re-seed, or fix migration).
  </action>
  <verify>
    - `npm run verify:corpus` exits with code 0 (all checks pass)
    - Output shows: ~90 transcripts, thousands of paragraphs, ~1.3M total words
    - All 5 search queries return results in under 200ms
    - Content hashes are all present and unique per transcript
    - Corpus repo file count matches database count
  </verify>
  <done>All 5 phase success criteria verified: (1) 90 transcripts with metadata in Supabase, (2) structured paragraphs with timestamps and speaker ID, (3) full-text search under 200ms, (4) content hashes for change detection, (5) corpus in separate repo</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete corpus foundation: ~90 McKenna transcripts scraped from organism.earth, stored as JSON in private GitHub repo, imported into Supabase with full-text search. Verification script confirms all phase success criteria are met.</what-built>
  <how-to-verify>
    1. Review the verification script output (should show all checks PASS)
    2. Open Supabase Dashboard -> Table Editor -> transcripts: confirm ~90 rows with metadata
    3. Open Supabase Dashboard -> Table Editor -> transcript_paragraphs: confirm thousands of rows
    4. Open Supabase Dashboard -> SQL Editor, run:
       ```sql
       SELECT * FROM search_paragraphs('novelty theory', 5);
       ```
       Confirm results appear with transcript titles and paragraph text
    5. Check GitHub: `mckenna-corpus` repo exists as private with JSON files
    6. Run `npm run seed` again: confirm all transcripts show as "unchanged, skipping"
  </how-to-verify>
  <resume-signal>Type "approved" if corpus looks correct, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
- Full corpus scraped and stored as JSON (~90 files)
- Private GitHub repo created with corpus data
- Supabase tables populated with all transcripts and paragraphs
- Full-text search returns results in under 200ms
- Verification script passes all checks
- User confirms data looks correct in Supabase Dashboard
</verification>

<success_criteria>
1. ~90 JSON files in private corpus repo committed to GitHub
2. ~90 transcripts in Supabase with all available metadata
3. Thousands of paragraphs with position, text, content_hash, and optional speaker/timestamp
4. 5 test searches complete in under 200ms with relevant results
5. Second seed run skips all transcripts (hash-based change detection works)
6. User approves data quality via Supabase Dashboard review
</success_criteria>

<output>
After completion, create `.planning/phases/01-corpus-foundation/01-04-SUMMARY.md`
</output>
