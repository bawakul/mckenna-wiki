# Phase 1.1: Corpus Data Fixes - Research

**Researched:** 2026-02-08
**Domain:** Web scraping metadata extraction, HTML parsing with Cheerio, data re-import
**Confidence:** HIGH

## Summary

Phase 1.1 fixes missing dates and topic tags in the McKenna transcript corpus that were discovered during Phase 3 UAT testing. The Phase 1 scraper implementation included placeholder code that set `date: null` and `topicTags: []` instead of extracting these values from organism.earth pages.

Investigation of saved HTML samples from organism.earth reveals that both dates and topic tags are present and consistently structured across transcript pages:
- **Dates** appear in an `<h3>` tag immediately after the title and subtitle headers
- **Topic tags** appear in a `<section id="topics">` containing links with class `metadata-label metadata-label-link`

The fix requires updating `parser.ts` to extract these fields using Cheerio selectors, then re-running the scraper to update JSON files, and re-seeding the database. The existing infrastructure (scraper, seed script, database schema) already supports these fields - only the extraction logic is missing.

**Primary recommendation:** Update parser.ts with two simple Cheerio selector additions, re-scrape all transcripts to update JSON files, then re-seed database using existing import-corpus.ts script.

## Standard Stack

### Core (Already Installed)
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| cheerio | 1.0.0+ | HTML parsing | Already in use; jQuery-like API for static HTML |
| @supabase/supabase-js | 2.x | Database client | Already in use for seed script |
| tsx | Latest | TypeScript execution | Already in use for running scrape/seed scripts |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| node:crypto | Built-in | SHA-256 hashing | Already in use for content hashing |

**Installation:**
No new dependencies required - all libraries already installed.

## Architecture Patterns

### Pattern 1: Extract from Consistent HTML Structure

**What:** Use specific Cheerio selectors to extract metadata from known HTML structure
**When to use:** Static HTML with consistent patterns across pages
**Why:** organism.earth uses consistent structure; selectors can be simple and reliable

**Example from organism.earth HTML:**
```html
<h1>Eros and the Eschaton</h1>
<h2>What Science Forgot: The Importance of Human Beings</h2>
<h3>March 25, 1994</h3>

<section id="topics">
  <a class="metadata-label metadata-label-link" href="...">
    Accelerating Change
  </a>
  <a class="metadata-label metadata-label-link" href="...">
    Cosmology
  </a>
  <!-- More topic tags -->
</section>
```

**Extraction code:**
```typescript
// Date extraction (after title/subtitle h1/h2)
const date = $('h3').first().text().trim() || null;

// Topic tags extraction (from topics section)
const topicTags: string[] = [];
$('section#topics a.metadata-label-link').each((_, el) => {
  const tag = $(el).text().trim();
  if (tag) topicTags.push(tag);
});
```

### Pattern 2: Re-scrape and Re-seed Workflow

**What:** Update scraper logic, re-run on all transcripts, re-import to database
**When to use:** Fixing extraction bugs in corpus scraping
**Why:** Maintains data integrity; batch re-import simpler than selective updates

**Workflow:**
```bash
# 1. Update parser.ts with new extraction logic
# 2. Test on sample to verify extraction
npm run scrape -- --limit 3 --output ./corpus-test

# 3. Verify JSON contains dates and tags
cat ./corpus-test/eros-and-the-eschaton.json | jq '.date, .topicTags'

# 4. Re-scrape full corpus
npm run scrape

# 5. Re-seed database (upsert will update existing records)
npm run seed
```

### Pattern 3: Content Hash Stability

**What:** Ensure content hash remains stable when metadata changes
**When to use:** Re-scraping with metadata fixes
**Why:** Content hash should only change when paragraph text changes, not when metadata is added

**Implementation note:**
Current hash-utils.ts hashes title + paragraphs only (not metadata fields), so adding dates/tags won't trigger false content changes.

### Anti-Patterns to Avoid

- **Selective manual updates:** Don't manually edit JSON files or database records - re-scrape ensures consistency
- **Migration instead of re-seed:** Don't write SQL migration to add dates/tags - re-seeding with upsert is cleaner
- **Overly complex date parsing:** organism.earth dates vary ("March 25, 1994", "June 1989", etc.) - store as-is per Phase 1 design decision

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Date normalization | Custom parser to convert "March 25, 1994" â†’ "1994-03-25" | Store as-is from source | Phase 1 decision: no inference/fabrication; varying granularity is valid data |
| Topic tag cleaning | Text preprocessing/normalization | .trim() only | organism.earth tags are already clean; over-processing risks data loss |
| Selective database updates | UPDATE queries for individual fields | Re-seed with upsert | Existing seed script handles this; maintains integrity |

**Key insight:** The scraper infrastructure (batch scrape, smart import via content hash) was designed for exactly this scenario. Don't work around it - use it.

## Common Pitfalls

### Pitfall 1: Assuming All Pages Have Dates

**What goes wrong:** Parser crashes or returns inconsistent data when some transcripts lack date fields

**Why it happens:** Not all organism.earth pages may have `<h3>` date tags; defensive coding needed

**How to avoid:**
- Use `|| null` fallback for date extraction
- Test against multiple samples including edge cases
- Log missing dates during scrape for visibility

**Warning signs:**
- Parser throws "undefined" errors
- Some transcripts have dates, others show "undefined" instead of null

### Pitfall 2: Topic Tag Duplicates

**What goes wrong:** Topic tags appear multiple times in array if selectors match multiple elements

**Why it happens:** Overly broad selector matches non-topic elements with same class

**How to avoid:**
- Use specific selector: `section#topics a.metadata-label-link`
- Test selector against HTML samples to verify precision
- Consider deduplication if organism.earth has duplicate tags

**Warning signs:**
- Topic arrays contain duplicates
- Non-topic elements (metadata labels) appear in tags

### Pitfall 3: Whitespace in Extracted Text

**What goes wrong:** Topic tags contain leading/trailing whitespace or newlines from HTML formatting

**Why it happens:** HTML indentation preserved by Cheerio .text()

**How to avoid:**
- Always .trim() extracted text
- Filter out empty strings before pushing to array

**Warning signs:**
- Tags display with extra spaces in UI
- Empty strings in topic_tags array

### Pitfall 4: Database Upsert Confusion

**What goes wrong:** Re-seeding creates duplicate records instead of updating existing ones

**Why it happens:** Upsert conflict resolution doesn't match on correct unique key

**How to avoid:**
- Verify seed script uses `{ onConflict: 'id' }` for transcripts
- Verify seed script uses `{ onConflict: 'transcript_id,position' }` for paragraphs
- Check database for duplicates after re-seed: `SELECT id, COUNT(*) FROM transcripts GROUP BY id HAVING COUNT(*) > 1`

**Warning signs:**
- Transcript count increases instead of staying at 90
- Search results show duplicate transcripts

### Pitfall 5: Testing with Limited Samples

**What goes wrong:** Extraction works on test samples but fails on edge cases in full corpus

**Why it happens:** Date formats vary ("March 1994", "1989", "March 25, 1994"); HTML structure may have variations

**How to avoid:**
- Test with --limit 10 spanning different years/formats before full scrape
- Log extraction results to verify date/tag presence across samples
- Review failed extractions in scraper output

**Warning signs:**
- Test succeeds but full scrape has many null dates
- Some transcripts have malformed topic tags

## Code Examples

### Date and Topic Tag Extraction (parser.ts update)

```typescript
// Source: Based on organism.earth HTML structure analysis
// Location: scripts/scrape/parser.ts, parseTranscriptPage function

// REPLACE lines 106-112 with:

// Extract date from <h3> tag (appears after h1 title and h2 subtitle)
// Format varies: "March 25, 1994", "June 1989", etc.
// Store as-is per Phase 1 decision (no normalization)
const dateElement = $('h3').first();
const date: string | null = dateElement.length > 0
  ? dateElement.text().trim()
  : null;

// Extract topic tags from topics section
// Structure: <section id="topics"><a class="metadata-label metadata-label-link">Tag Name</a></section>
const topicTags: string[] = [];
$('section#topics a.metadata-label-link').each((_, el) => {
  const tag = $(el).text().trim();
  if (tag) {
    topicTags.push(tag);
  }
});

// referencedAuthors remains empty for now (not part of this phase)
const referencedAuthors: string[] = [];
```

### Test Extraction on Single Transcript

```bash
# Source: Development testing pattern
# Run scraper on single transcript to verify extraction

# Test with known transcript
npm run scrape -- --limit 1

# Verify output
cat corpus/transcripts/eros-and-the-eschaton.json | jq '{date, topicTags}'

# Expected output:
# {
#   "date": "March 25, 1994",
#   "topicTags": [
#     "Accelerating Change",
#     "Cosmology",
#     "Entropy and Syntropy",
#     "Evolution",
#     "Omega Point",
#     "Psychedelics",
#     "Technology"
#   ]
# }
```

### Re-seed Database with Updated Data

```bash
# Source: Existing seed workflow
# Re-import corpus after re-scraping

# 1. Re-scrape all transcripts
npm run scrape

# 2. Re-seed database (upserts will update existing records)
npm run seed

# 3. Verify in database
# Run in Supabase SQL editor or psql:
SELECT
  id,
  date,
  array_length(topic_tags, 1) as tag_count,
  topic_tags[1:3] as sample_tags
FROM transcripts
WHERE date IS NOT NULL
ORDER BY date DESC
LIMIT 10;
```

### Verification Query - Check Coverage

```sql
-- Source: Data quality verification pattern
-- Check how many transcripts now have dates and tags

SELECT
  COUNT(*) as total_transcripts,
  COUNT(CASE WHEN date IS NOT NULL THEN 1 END) as with_dates,
  COUNT(CASE WHEN array_length(topic_tags, 1) > 0 THEN 1 END) as with_tags,
  ROUND(100.0 * COUNT(CASE WHEN date IS NOT NULL THEN 1 END) / COUNT(*), 1) as date_coverage_pct,
  ROUND(100.0 * COUNT(CASE WHEN array_length(topic_tags, 1) > 0 THEN 1 END) / COUNT(*), 1) as tag_coverage_pct
FROM transcripts;

-- Expected: High coverage (>90%) if organism.earth is consistent
```

## State of the Art

No state-of-the-art changes apply to this phase - using existing stable patterns:

| Old Approach | Current Approach | Notes |
|--------------|------------------|-------|
| N/A | Cheerio selectors + batch re-scrape | Pattern established in Phase 1 |
| N/A | Upsert-based re-import | Pattern established in Phase 1 |

## Open Questions

### Question 1: Date Format Variation Across Corpus

**What we know:**
- Sample 1: "March 25, 1994" (full date)
- Sample 2: "June 1989" (month and year)
- Phase 1 decision: store as-is, don't normalize

**What's unclear:**
- Are there year-only dates ("1994")?
- Are there transcripts without dates at all?
- What percentage of corpus has full dates vs partial dates?

**Recommendation:**
- Log date extraction results during scraping to analyze coverage
- Accept all formats as-is per Phase 1 decision
- Document coverage in verification step

### Question 2: Topic Tag Consistency

**What we know:**
- Tags appear in `<section id="topics">` with consistent HTML structure
- Sample tags: "Accelerating Change", "Cosmology", "Psychedelics", etc.

**What's unclear:**
- Are all 90 transcripts tagged?
- What's the full set of unique tags across corpus?
- Are there transcripts with no tags?

**Recommendation:**
- Verify coverage after re-scrape
- Generate tag frequency report for insight
- Empty tag arrays are valid (per Phase 1 "no fabrication" principle)

### Question 3: Referenced Authors Field

**What we know:**
- Database schema includes `referenced_authors` field
- UAT focused on dates and tags (not authors)
- Sample HTML shows author links in separate section

**What's unclear:**
- Is referenced_authors extraction required for Phase 1.1?
- Is there a separate section for referenced authors in organism.earth HTML?

**Recommendation:**
- Phase 1.1 focuses only on dates and tags per success criteria
- Leave `referencedAuthors: []` as-is
- Defer author extraction to future phase if needed

## Sources

### Primary (HIGH confidence)
- Local HTML samples: `scripts/scrape/samples/mckenna-doc-1.html` - Direct HTML structure inspection
- Local HTML samples: `scripts/scrape/samples/mckenna-doc-2.html` - Pattern verification across multiple pages
- Phase 1 Research: `.planning/phases/01-corpus-foundation/01-RESEARCH.md` - Established scraping patterns
- Phase 1 Context: `.planning/phases/01-corpus-foundation/01-CONTEXT.md` - Design decisions (store as-is, no normalization)
- Current implementation: `scripts/scrape/parser.ts` - Existing extraction code with placeholder lines 108, 111

### Secondary (MEDIUM confidence)
- Phase 3 UAT: `.planning/phases/03-reading-interface/03-UAT.md` - Issues discovered (dates missing, tags missing)
- Database schema: `supabase/migrations/001_create_corpus_tables.sql` - Confirmed date and topic_tags fields exist

## Metadata

**Confidence breakdown:**
- HTML structure: HIGH - Direct inspection of saved samples confirms consistent patterns
- Extraction approach: HIGH - Cheerio selectors straightforward for static HTML structure
- Re-scrape workflow: HIGH - Existing infrastructure designed for this scenario
- Database update: HIGH - Upsert pattern already implemented and tested in Phase 1

**Research date:** 2026-02-08
**Valid until:** 2026-04-08 (60 days - organism.earth HTML structure may change, but unlikely for static content site)

**Critical finding:** No new libraries or complex logic needed. Phase 1 infrastructure was built correctly; only the extraction selectors were left as TODOs. This is a 2-line code change + re-scrape + re-seed workflow.
