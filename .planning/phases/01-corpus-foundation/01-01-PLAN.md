---
phase: 01-corpus-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - tsconfig.json
  - next.config.ts
  - .env.local
  - .env.example
  - .gitignore
  - scripts/scrape/explore.ts
  - scripts/scrape/SELECTORS.md
autonomous: true

must_haves:
  truths:
    - "Next.js project initializes and builds without errors"
    - "All required dependencies (cheerio, axios, supabase-js, zod, p-limit) are installed"
    - "Organism.earth HTML structure is documented with working CSS selectors for title, date, paragraphs, speakers, timestamps, and metadata"
    - "At least 3 sample transcripts are successfully scraped and their structure is understood"
  artifacts:
    - path: "package.json"
      provides: "Project dependencies and scripts"
      contains: "cheerio"
    - path: "scripts/scrape/explore.ts"
      provides: "Exploratory scraping script with documented findings"
      min_lines: 50
    - path: "scripts/scrape/SELECTORS.md"
      provides: "Documented HTML selectors and structure reference"
      min_lines: 20
  key_links:
    - from: "scripts/scrape/explore.ts"
      to: "organism.earth"
      via: "axios HTTP request"
      pattern: "axios\\.get.*organism\\.earth"
---

<objective>
Initialize the Next.js project with all Phase 1 dependencies and explore organism.earth HTML structure by scraping 3-5 sample transcripts.

Purpose: Establish the project foundation and resolve the biggest unknown -- the exact HTML structure of organism.earth transcript pages. Every subsequent plan depends on knowing the CSS selectors, metadata locations, and paragraph structure.

Output: Working project with dependencies, documented HTML selectors, and sample transcript data to inform scraper and schema design.
</objective>

<execution_context>
@/Users/bharadwajkulkarni/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bharadwajkulkarni/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-corpus-foundation/01-CONTEXT.md
@.planning/phases/01-corpus-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize Next.js project with Phase 1 dependencies</name>
  <files>package.json, tsconfig.json, next.config.ts, .env.local, .env.example, .gitignore</files>
  <action>
    1. Run `npx create-next-app@latest . --typescript --tailwind --eslint --app --src-dir --import-alias "@/*" --use-npm` in the project root (the directory already has .planning/ and .git, so create-next-app should work in existing directory -- if it refuses, use `--yes` or create in a temp dir and move files).

    2. Install Phase 1 dependencies:
       ```
       npm install cheerio axios @supabase/supabase-js
       npm install --save-dev zod p-limit tsx @types/node
       ```
       Note: `tsx` is for running TypeScript scripts directly (e.g., `npx tsx scripts/scrape/explore.ts`).
       Note: `p-limit` is ESM-only (v5+). If import issues arise, use v4 which supports CommonJS.

    3. Create directory structure for scripts:
       ```
       scripts/
       ├── scrape/
       └── seed/
       ```

    4. Create `.env.example` with placeholder variables:
       ```
       SUPABASE_URL=your-supabase-url
       SUPABASE_ANON_KEY=your-supabase-anon-key
       SUPABASE_SERVICE_KEY=your-supabase-service-key
       CORPUS_REPO_PATH=./mckenna-corpus
       ```

    5. Create `.env.local` with same structure (will be gitignored).

    6. Ensure `.gitignore` includes `.env.local`, `node_modules/`, and `.next/`.

    7. Add a script entry to package.json: `"scrape:explore": "npx tsx scripts/scrape/explore.ts"`

    8. Verify the project builds: `npm run build`
  </action>
  <verify>
    - `npm run build` completes without errors
    - `npm ls cheerio axios @supabase/supabase-js` shows all installed
    - `ls scripts/scrape scripts/seed` shows both directories exist
    - `.env.example` exists with the four variables
  </verify>
  <done>Next.js project initialized with all Phase 1 dependencies, directory structure created, build passes</done>
</task>

<task type="auto">
  <name>Task 2: Explore organism.earth HTML structure</name>
  <files>scripts/scrape/explore.ts, scripts/scrape/SELECTORS.md</files>
  <action>
    1. Create `scripts/scrape/explore.ts` -- an exploratory script that:
       - Fetches the organism.earth transcript index page to discover transcript URLs
         - Try these URLs: `https://www.organism.earth/library`, `https://organism.earth/library`, `https://www.organism.earth/library/document`
         - If index page not found, try to find transcript links from the homepage
       - Fetches 3-5 individual transcript pages
       - For each page, logs:
         - The page title element and its selector
         - All metadata elements (date, location, speakers, duration, word count, topic tags, referenced authors)
         - Paragraph structure (how paragraphs are wrapped, any data attributes)
         - Timestamp format and location (data attribute? text content? separate element?)
         - Speaker identification format
         - Any other relevant structure (summary/description, related links, etc.)
       - Saves raw HTML of each sample page to `scripts/scrape/samples/` for offline analysis
       - Uses polite scraping: 3-second delays between requests, descriptive User-Agent

    2. Run the exploratory script: `npx tsx scripts/scrape/explore.ts`

    3. Analyze the output and create `scripts/scrape/SELECTORS.md` documenting:
       - Index page URL and how transcript URLs are discovered
       - For each metadata field: exact CSS selector, example value, whether always present
       - Paragraph container selector
       - Timestamp selector/format
       - Speaker identification selector/format
       - Any unexpected structure or edge cases discovered
       - Notes on which fields are present vs missing across the samples

    4. Add `scripts/scrape/samples/` to `.gitignore` (raw HTML shouldn't be committed).

    IMPORTANT: If organism.earth is unreachable or blocks scraping, document this in SELECTORS.md and note what alternative approaches might work (different User-Agent, checking robots.txt, etc.). Do NOT fabricate selectors -- real testing is essential.
  </action>
  <verify>
    - `scripts/scrape/explore.ts` exists and runs without crashing
    - `scripts/scrape/SELECTORS.md` exists with documented selectors (or documented blockers if site is unreachable)
    - At least 3 sample HTML files saved in `scripts/scrape/samples/`
    - Output includes working selectors for title, paragraphs, and at least 3 metadata fields
  </verify>
  <done>Organism.earth HTML structure is documented with working CSS selectors for all required data fields (or blockers are documented if site is unreachable)</done>
</task>

</tasks>

<verification>
- Project builds cleanly: `npm run build`
- Dependencies installed: `npm ls cheerio axios @supabase/supabase-js zod p-limit tsx`
- Exploratory script runs: `npx tsx scripts/scrape/explore.ts`
- SELECTORS.md has actionable information for building the scraper
</verification>

<success_criteria>
1. `npm run build` passes
2. All 6 dependencies (cheerio, axios, @supabase/supabase-js, zod, p-limit, tsx) installed
3. SELECTORS.md contains documented CSS selectors for organism.earth transcript pages
4. At least 3 sample transcripts explored with structure documented
</success_criteria>

<output>
After completion, create `.planning/phases/01-corpus-foundation/01-01-SUMMARY.md`
</output>
