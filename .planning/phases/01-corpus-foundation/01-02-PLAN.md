---
phase: 01-corpus-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - scripts/scrape/types.ts
  - scripts/scrape/parser.ts
  - scripts/scrape/scraper.ts
  - scripts/scrape/hash-utils.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Scraper extracts title, date, location, speakers, duration, word count, topic tags, and referenced authors from organism.earth transcript pages"
    - "Scraper structures transcript text into paragraphs with position index, content hash, speaker, and timestamp"
    - "Scraper respects organism.earth with 2-3 second delays between requests"
    - "Test scrape of 5-10 transcripts produces valid JSON files matching the defined schema"
  artifacts:
    - path: "scripts/scrape/types.ts"
      provides: "TypeScript types and Zod schemas for transcript data"
      exports: ["TranscriptSchema", "ParagraphSchema", "Transcript", "Paragraph"]
      min_lines: 40
    - path: "scripts/scrape/parser.ts"
      provides: "HTML parsing logic using documented selectors"
      exports: ["parseTranscriptPage", "parseTranscriptIndex"]
      min_lines: 80
    - path: "scripts/scrape/scraper.ts"
      provides: "Main scraper orchestrator with politeness and error handling"
      min_lines: 80
    - path: "scripts/scrape/hash-utils.ts"
      provides: "SHA-256 hashing for transcripts and paragraphs"
      exports: ["hashTranscriptContent", "hashParagraph"]
      min_lines: 20
  key_links:
    - from: "scripts/scrape/parser.ts"
      to: "scripts/scrape/types.ts"
      via: "imports Transcript and Paragraph types"
      pattern: "import.*from.*types"
    - from: "scripts/scrape/scraper.ts"
      to: "scripts/scrape/parser.ts"
      via: "calls parseTranscriptPage for each URL"
      pattern: "parseTranscriptPage"
    - from: "scripts/scrape/scraper.ts"
      to: "scripts/scrape/hash-utils.ts"
      via: "generates content hashes for each transcript and paragraph"
      pattern: "hashTranscriptContent|hashParagraph"
---

<objective>
Build the complete scraper pipeline: TypeScript types with Zod validation, HTML parser using documented selectors, and scraper orchestrator with politeness delays and error handling. Validate by scraping 5-10 transcripts into JSON files.

Purpose: This is the core data acquisition tool. The scraper must reliably extract all metadata and structured paragraph text from organism.earth, producing validated JSON files that the corpus repo and seed script will consume.

Output: Working scraper that can scrape any organism.earth transcript into a validated JSON file, tested against 5-10 real transcripts.
</objective>

<execution_context>
@/Users/bharadwajkulkarni/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bharadwajkulkarni/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-corpus-foundation/01-CONTEXT.md
@.planning/phases/01-corpus-foundation/01-RESEARCH.md
@.planning/phases/01-corpus-foundation/01-01-SUMMARY.md
@scripts/scrape/SELECTORS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define TypeScript types, Zod schemas, and HTML parser</name>
  <files>scripts/scrape/types.ts, scripts/scrape/parser.ts, scripts/scrape/hash-utils.ts</files>
  <action>
    1. Create `scripts/scrape/types.ts` with:
       - `Paragraph` interface: position (number), speaker (string | null), timestamp (string | null), text (string), contentHash (string)
       - `Transcript` interface: id (string, derived from URL slug), url (string), title (string), date (string | null), location (string | null), speakers (string[]), durationMinutes (number | null), wordCount (number | null), topicTags (string[]), referencedAuthors (string[]), description (string | null), paragraphs (Paragraph[]), contentHash (string), scrapedAt (string, ISO timestamp)
       - Corresponding Zod schemas (ParagraphSchema, TranscriptSchema) for runtime validation
       - Export all types and schemas

    2. Create `scripts/scrape/hash-utils.ts` with:
       - `hashParagraph(text: string): string` -- SHA-256 of normalized text (trimmed, collapsed whitespace), first 16 hex chars
       - `hashTranscriptContent(title: string, paragraphs: Array<{text: string}>): string` -- SHA-256 of JSON.stringify({title: title.trim(), paragraphs: paragraphs.map(p => p.text.trim())})
       - Uses Node.js built-in `crypto.createHash('sha256')`

    3. Create `scripts/scrape/parser.ts` with:
       - `parseTranscriptIndex(html: string): string[]` -- extracts transcript URLs from the index/listing page. Use selectors documented in SELECTORS.md from Plan 01.
       - `parseTranscriptPage(html: string, url: string): Omit<Transcript, 'contentHash' | 'scrapedAt'>` -- extracts all metadata and paragraphs from a single transcript page.
         - Use selectors from SELECTORS.md
         - For each paragraph: extract text, speaker (if identified), timestamp (if present), compute contentHash
         - Position is zero-indexed sequential order
         - Store dates as-is from source (don't normalize partial dates)
         - Missing metadata fields -> null (not undefined)
         - Speaker identification: use actual names when identified, fall back to "Audience" / "Questioner" for unidentified speakers
       - Validate parsed output against Zod schema before returning

    IMPORTANT: The parser MUST use the actual selectors discovered in Plan 01 (documented in SELECTORS.md). Do NOT guess selectors. Read SELECTORS.md and use those exact selectors.
  </action>
  <verify>
    - `npx tsx -e "import { TranscriptSchema } from './scripts/scrape/types'; console.log('Types OK')"` runs without error
    - `npx tsx -e "import { hashParagraph } from './scripts/scrape/hash-utils'; console.log(hashParagraph('test'))"` outputs a 16-char hex string
    - `npx tsx -e "import { parseTranscriptPage } from './scripts/scrape/parser'; console.log('Parser OK')"` imports without error
    - Parser functions use selectors from SELECTORS.md (grep for actual CSS selectors used)
  </verify>
  <done>TypeScript types, Zod schemas, hash utilities, and HTML parser are complete with selectors from exploratory scraping</done>
</task>

<task type="auto">
  <name>Task 2: Build scraper orchestrator and test with 5-10 transcripts</name>
  <files>scripts/scrape/scraper.ts, package.json</files>
  <action>
    1. Create `scripts/scrape/scraper.ts` -- the main orchestrator:
       - Accepts optional CLI args: `--limit N` (scrape first N transcripts, default all), `--output DIR` (output directory, default from CORPUS_REPO_PATH env var or `./corpus/transcripts`)
       - Workflow:
         a. Fetch transcript index page -> extract all transcript URLs using parseTranscriptIndex
         b. Log total count found (expect ~90)
         c. For each URL (respecting --limit):
            - Fetch page with 2-3 second delay (use p-limit with concurrency 1)
            - Parse with parseTranscriptPage
            - Compute contentHash using hashTranscriptContent
            - Add scrapedAt timestamp (ISO format)
            - Validate against TranscriptSchema with Zod
            - Write JSON file to output directory: `{id}.json` (pretty-printed with 2-space indent)
            - Log success: transcript title, paragraph count, word count
         d. On individual transcript failure: log error with URL, continue to next (don't abort)
         e. At end: log summary (total scraped, total failed, total paragraphs, total words)
       - HTTP config: descriptive User-Agent header, responseEncoding 'utf-8', timeout 30 seconds
       - Retry logic: on 429 or 5xx, retry up to 3 times with exponential backoff (2s, 4s, 8s)

    2. Add package.json scripts:
       ```
       "scrape": "npx tsx scripts/scrape/scraper.ts",
       "scrape:test": "npx tsx scripts/scrape/scraper.ts --limit 10"
       ```

    3. Run test scrape: `npm run scrape:test`
       - Verify 5-10 JSON files are created in the output directory
       - Spot-check 2-3 JSON files: do they have title, paragraphs, metadata?
       - Verify Zod validation passes (no validation errors in logs)
       - Check content hashes are consistent (same transcript scraped twice should produce same hash)

    4. Create the output directory if it doesn't exist. Ensure the output directory is gitignored if it's inside the project (add to .gitignore if needed).

    IMPORTANT: If organism.earth blocks or is unreachable during the test scrape, document the specific error and any workarounds attempted. Do not fabricate sample data.
  </action>
  <verify>
    - `npm run scrape:test` completes without crashing
    - At least 5 JSON files exist in the output directory
    - Each JSON file passes Zod schema validation
    - JSON files contain: id, title, paragraphs array with position/text/contentHash
    - Scraper logs show polite delays between requests (2-3 second gaps)
    - Failed transcripts (if any) are logged but don't stop the batch
  </verify>
  <done>Scraper orchestrator works end-to-end: fetches index, scrapes individual transcripts with politeness delays, outputs validated JSON files. Test scrape of 5-10 transcripts produces correct output.</done>
</task>

</tasks>

<verification>
- All 4 source files exist and import correctly: types.ts, parser.ts, hash-utils.ts, scraper.ts
- `npm run scrape:test` produces 5-10 valid JSON files
- JSON files match the defined schema (Zod validates without errors)
- Each JSON file has: id, title, date, paragraphs[], contentHash, scrapedAt
- Each paragraph has: position, text, contentHash, speaker (or null), timestamp (or null)
- Scraper respects rate limiting (2-3 second delays visible in logs)
</verification>

<success_criteria>
1. Types, parser, hash utils, and scraper are complete and passing
2. Test scrape of 5-10 transcripts produces valid JSON files
3. Metadata extraction works for all fields that organism.earth provides
4. Paragraph structure preserved with position indexes and content hashes
5. Error handling works (failed transcripts logged, batch continues)
</success_criteria>

<output>
After completion, create `.planning/phases/01-corpus-foundation/01-02-SUMMARY.md`
</output>
